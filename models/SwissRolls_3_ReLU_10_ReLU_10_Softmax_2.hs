{-@ LIQUID "--reflection" @-}
{-@ LIQUID "--ple"        @-}

module SwissRolls_3_ReLU_10_ReLU_10_Softmax_2 where

import           Lazuli.LinearAlgebra
import qualified Lazuli.LinearAlgebra.Internal
import           Lazuli.Network
import qualified Lazuli.Prelude

{-@ reflect layer0 @-}
{-@ layer0 :: LayerN 3 10 @-}
layer0 :: Layer
layer0 = Layer
  { weights    = (3 >< 10)
                 [ 10 |> [-0.57913232, -0.35667667, -0.18435630, -0.44258603, -0.49621508, 0.43316501, 0.13085549, -0.31970328, -0.08271330, 0.47743058]
                 , 10 |> [0.66387320, 0.56616360, -0.42358020, 0.54646558, 0.29050496, 0.08301319, 0.00256489, 0.11889816, -0.62656933, 0.42376849]
                 , 10 |> [0.33045816, -0.48957458, 0.50671589, -0.25183654, -0.65023601, -0.70406240, -0.60377651, -0.10305993, 0.02793148, -0.59834826]
                 ]
  , biases     = 10 :> [-0.02699302, 0.12669042, -0.05153202, -0.20873931, 0.13237359, 0.19496976, -0.18631668, -0.14257000, -0.01621597, -0.11743411]
  , activation = ReLU
  }

{-@ reflect layer1 @-}
{-@ layer1 :: LayerN 10 10 @-}
layer1 :: Layer
layer1 = Layer
  { weights    = (10 >< 10)
                 [ 10 |> [0.19943254, 0.21449959, -0.36367607, 0.49790242, -0.07719702, -0.48754740, -0.13401362, 0.16231053, 0.45041168, 0.19623195]
                 , 10 |> [0.24372886, 0.21123680, 0.16629243, -0.25703594, 0.41424909, -0.18142852, 0.40323183, -0.24515559, -0.30533999, 0.43326026]
                 , 10 |> [0.54928923, -0.52871382, -0.42929223, 0.17175740, 0.28727844, 0.15152653, -0.24430783, 0.07239076, 0.41747376, 0.17618510]
                 , 10 |> [0.28875613, -0.41361430, -0.27544141, -0.42797175, 0.00094479, 0.52960861, -0.10950962, 0.03656391, -0.47742337, 0.45867607]
                 , 10 |> [0.39696231, 0.18691152, -0.39029092, -0.27939966, 0.50774568, 0.07789826, -0.50343502, 0.02197306, -0.56180990, -0.40641615]
                 , 10 |> [0.12235828, -0.10927629, -0.32463321, 0.47433749, 0.21401511, -0.07821393, -0.18358196, -0.34784943, -0.37677953, -0.02377604]
                 , 10 |> [0.12061915, -0.48573226, 0.11845165, 0.29693928, -0.46429759, -0.17755085, -0.13046768, 0.36515510, -0.23692417, -0.25804684]
                 , 10 |> [0.00840393, -0.37436941, 0.27197981, 0.16207194, -0.37366939, -0.51281440, 0.34614009, -0.31727675, 0.57258725, 0.29610682]
                 , 10 |> [-0.27976495, -0.32787988, -0.36247951, 0.49315253, 0.42625430, 0.49017221, -0.11047864, -0.17888555, 0.32999203, -0.15494843]
                 , 10 |> [0.33791211, -0.08084633, -0.19647086, -0.43313959, 0.08507902, -0.51967853, 0.08370546, -0.32736230, 0.00479679, 0.22204085]
                 ]
  , biases     = 10 :> [-0.19963405, 0.14998969, 0.00000000, 0.10165567, 0.22187802, -0.09847988, -0.10405608, -0.04179272, 0.05627026, 0.24625695]
  , activation = ReLU
  }

{-@ reflect layer2 @-}
{-@ layer2 :: LayerN 10 2 @-}
layer2 :: Layer
layer2 = Layer
  { weights    = (10 >< 2)
                 [ 2 |> [0.47929376, 0.02720942]
                 , 2 |> [-0.38901135, 0.23608859]
                 , 2 |> [-0.14956182, -0.30998719]
                 , 2 |> [-0.38275379, -0.19909520]
                 , 2 |> [-0.66221118, -0.03538330]
                 , 2 |> [0.13841312, 0.51486653]
                 , 2 |> [0.11653873, -0.64725935]
                 , 2 |> [0.54221421, -0.67865938]
                 , 2 |> [-0.54374862, -0.48299375]
                 , 2 |> [-0.18605201, -0.12465122]
                 ]
  , biases     = 2 :> [-0.20647727, 0.20647727]
  , activation = Softmax
  }

{-@ reflect model @-}
{-@ model :: NetworkN 3 2 @-}
model :: Network
model = NStep layer0 (NStep layer1 (NLast layer2))